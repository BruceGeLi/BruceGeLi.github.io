---
title: "This is Ge Li (Bruce)"
#permalink: /:title/
layout: page
---

<img src="/assets/images/Profile.gif" alt="Alt text" style="float: left; margin-right: 30px; margin-top: 10px; margin-bottom: 20px;" width="40%" height="40%" />
I am a Ph.D. candidate in machine learning and robotics at the Karlsruhe Institute of Technology (KIT), Germany. My supervisors are <a href="https://scholar.google.de/citations?user=GL360kMAAAAJ&hl=en" target="_blank" rel="noopener noreferrer">Prof. Gerhard Neumann</a> and
<a href="https://rudolf.intuitive-robots.net/" target="_blank" rel="noopener noreferrer">Prof. Rudolf Lioutikov</a>.

I focus on advancing the learning and representation of robotic motion policies, 
with a particular emphasis on their applications in **imitation and reinforcement learning**.
I aim to improve **model capacity, learning efficiency**, and **interpretability** by leveraging
state-of-the-art algorithms and tools. 

In addition to my research, I serve as a teaching assistant at KIT, contributing to courses 
**Cognitive Systems**, **Machine Learning**, and **Deep Reinforcement Learning**. I also mentor several Bachelor and Master students for their research projects and theses.

---

# Education & Experience

2020 - Present, <a href="https://www.kit.edu/english/" target="_blank" rel="noopener noreferrer">Karlsruhe Institute of Technology, Germany</a>, **Ph.D. Candidate, Computer Science**.

2015 - 2019, <a href="https://www.rwth-aachen.de/" target="_blank" rel="noopener noreferrer">RWTH Aachen University, Germany</a>, **Master's Degree**, Computer-Aided Mechanical Engineering.

2011 - 2015, <a href="https://en.ustc.edu.cn/" target="_blank" rel="noopener noreferrer">University of Science and Technology of China</a>, **Bachelor's Degree**, Mechanical Engineering. 

2018 - 2019, <a href="https://is.mpg.de/" target="_blank" rel="noopener noreferrer">Max Planck Institute for Intelligent Systems</a>, Germany, Robot Research Intern.

2018, <a href="https://www.kaercher.com/int/" target="_blank" rel="noopener noreferrer">Alfred KÃ¤rcher SE & Co. KG</a>, Germany, Robot Software Engineer Intern.

---

# News

Jan. 2025, my latest work got accepted by ICLR 2025 as a Spotlight paperðŸ”¥. I will present it in Singapore ðŸ‡¸ðŸ‡¬.

Dec. 2024, I presented my co-authored work in NeurIPS 2024 in Vancouver, Canada ðŸ‡¨ðŸ‡¦.

Aug. 2024, one co-authored paper got accepted by CoRL 2024 ðŸ‡©ðŸ‡ª.

May 2024, I presented my PhD work in ICLR 2024 in Vienna, Austria ðŸ‡¦ðŸ‡¹.

---

# Publications

[//]: # (TOP-ERL)
<div style="display: flex; align-items: flex-start;">
  <div style="flex: 1; padding-right: 20px; justify-content: space-between; padding-top: 10px; align-items: flex-start;">
    <a href="/assets/images/TOP_ERL_critic.png" target="_blank" rel="noopener noreferrer">
    <img src="/assets/images/TOP_ERL_critic.png" alt="TCE" />
    </a>
    <a href="/assets/images/TOP_ERL_results.png" target="_blank" rel="noopener noreferrer">
    <img src="/assets/images/TOP_ERL_results.png" alt="TCE" />
    </a> 
  </div>
  <div style="flex: 2;">
    <p>
        <strong>TOP-ERL: Transformer-based Off-Policy Episodic Reinforcement Learning</strong>
    </p>
    <p>
        <strong>Ge Li</strong>, Dong Tian, Hongyi Zhou, Xinkai Jiang, Rudolf Lioutikov, Gerhard Neumann, <strong>ICLR, 2025 SpotlightðŸ”¥ (top 5%)</strong>.
    </p>
    <p>
        See:
        <a href="https://arxiv.org/pdf/2410.09536" target="_blank" rel="noopener noreferrer">arxiv</a>
    </p>
    <p>
        We propose a novel online off-policy RL methodology that utilizes a transformer-based critic to learn values of action sequences.
    </p>
  </div>
</div>

<br><br>


[//]: # (DIME)
<div style="display: flex; align-items: flex-start;">
  <div style="flex: 1; padding-right: 20px; justify-content: space-between; padding-top: 10px; align-items: flex-start;">
    <a href="/assets/images/dime_env.png" target="_blank" rel="noopener noreferrer">
    <img src="/assets/images/dime_env.png" alt="DIME" />
    </a>
    <a href="/assets/images/dime_exp.png" target="_blank" rel="noopener noreferrer">
    <img src="/assets/images/dime_exp.png" alt="DIME" />
    </a>
  </div>
  <div style="flex: 2;">
    <p>
        <strong>DIME: Diffusion-Based Maximum Entropy Reinforcement Learning</strong>
    </p>
    <p>
        Onur Celik, Zechu Li, Denis Blessing, <strong>Ge Li</strong>, Daniel Palenicek, 
        <a href="https://scholar.google.com/citations?hl=en&user=-kIVAcAAAAAJ&view_op=list_works&sortby=pubdate" target="_blank" rel="noopener noreferrer"> Jan Peters</a>, 
        <a href="https://pearl-lab.com/people/georgia-chalvatzaki/" rel="noopener noreferrer"> Georgia Chalvatzaki</a>, 
        Gerhard Neumann, <strong>Preprint, 2025</strong>.
    </p>
    <p>
        See:
        <a href="https://arxiv.org/pdf/2502.02316" target="_blank" rel="noopener noreferrer">arxiv</a>
    </p>
    <p>
        The SOTA diffusion-based online RL method that outperforms other cutting-edge diffusion and Gaussian policy methods.
    </p>
  </div>
</div>

<br><br>



[//]: # (IRIS)
<div style="display: flex; align-items: flex-start;">
  <div style="flex: 1; padding-right: 20px; justify-content: space-between; padding-top: 10px; align-items: flex-start;">
    <a href="/assets/images/IRIS_TT.gif" target="_blank" rel="noopener noreferrer">
    <img src="/assets/images/IRIS_TT.gif" alt="IRIS_TT"/>
    </a>
    <a href="/assets/images/IRIS_UNITREE.gif" target="_blank" rel="noopener noreferrer">
    <img src="/assets/images/IRIS_UNITREE.gif" alt="IRIS_UNITREE"/>
    </a> 
  </div>
  <div style="flex: 2;">
    <p>
        <strong>IRIS: An Immersive Robot Interaction System</strong>
    </p>
    <p>
        Xinkai Jiang, Qihao Yuan, Enes Ulas Dincer, Hongyi Zhou, <strong>Ge Li</strong>, Xueyin Li,
        Julius Haag, Nicolas Schreiber, Kailai Li, Gerhard Neumann, Rudolf Lioutikov, 
        <strong>Preprint, 2025</strong>.
    </p>
    <p>
        See:
        <a href="https://arxiv.org/pdf/2502.03297" target="_blank" rel="noopener noreferrer">arxiv</a> |
        <a href="https://intuitive-robots.github.io/iris-project-page/" target="_blank" rel="noopener noreferrer">Website</a>
    </p>
    <p>
        We present IRIS, an immersive Robot Interaction System leveraging Extended Reality (XR), 
        designed for robot data collection and interaction across multiple simulators, benchmarks, and real-world scenarios
    </p>
  </div>
</div>

<br><br>

[//]: # (VDD)
<div style="display: flex; align-items: flex-start;">
  <div style="flex: 1; padding-right: 20px; justify-content: space-between; padding-top: 10px; align-items: flex-start;">
    <a href="/assets/images/VDD_overview.png" target="_blank" rel="noopener noreferrer">
    <img src="/assets/images/VDD_overview.png" alt="TCE" />
    </a>
    <a href="/assets/images/vdd_toy_task_animation.gif" target="_blank" rel="noopener noreferrer">
    <img src="/assets/images/vdd_toy_task_animation.gif" alt="VDD_toy" style="width: 52%; max-width: 300px;"/>
    </a>
    <a href="/assets/images/vdd_sort_6.gif" target="_blank" rel="noopener noreferrer">
    <img src="/assets/images/vdd_sort_6.gif" alt="VDD_toy" style="width: 46%; max-width: 300px;"/>
    </a>
  </div>
  <div style="flex: 2;">
    <p>
        <strong>Variational Distillation of Diffusion Policies into Mixture of Experts</strong>
    </p>
    <p>
        Hongyi Zhou, Denis Blessing, <strong>Ge Li</strong>, Onur Celik, Gerhard Neumann, Rudolf Lioutikov, <strong>NeurIPS, 2024</strong>.
    </p>
    <p>
        See:
        <a href="https://arxiv.org/pdf/2406.12538" target="_blank" rel="noopener noreferrer">arxiv</a> | 
        <a href="https://neurips.cc/virtual/2024/poster/93992" target="_blank" rel="noopener noreferrer">NeurIPS</a> | 
        <a href="https://intuitive-robots.github.io/vdd-website/" target="_blank" rel="noopener noreferrer">GitHub</a>   
    </p>
    <p>
        We introduce Variational Diffusion Distillation (VDD), a novel method for distilling denoising diffusion policies into a Mixture of Experts (MoE).
    </p>
  </div>
</div>

<br><br>


[//]: # (Mamba)
<div style="display: flex; align-items: flex-start;">
  <div style="flex: 1; padding-right: 20px; justify-content: space-between; padding-top: 10px; align-items: flex-start;">
    <a href="/assets/images/Mamba_architecture.png" target="_blank" rel="noopener noreferrer">
    <img src="/assets/images/Mamba_architecture.png" alt="TCE" />
    </a>
    <a href="/assets/images/Mamba_architecture2.png.png" target="_blank" rel="noopener noreferrer">
    <img src="/assets/images/Mamba_architecture2.png" alt="TCE" />
    </a>
    <a href="/assets/images/Mamba_cups.gif" target="_blank" rel="noopener noreferrer">
    <img src="/assets/images/Mamba_cups.gif" alt="VDD_toy" style="width: 47.5%; max-width: 300px;"/>
    </a>
    <a href="/assets/images/Mamba_insert.gif" target="_blank" rel="noopener noreferrer">
    <img src="/assets/images/Mamba_insert.gif" alt="VDD_toy" style="width: 48.5%; max-width: 300px;"/>
    </a>
  </div>
  <div style="flex: 2;">
    <p>
        <strong>MaIL: Improving Imitation Learning with Selective State Space Models</strong>
    </p>
    <p>
        Xiaogang Jia, Qian Wang, Atalay Donat, Bowen Xing, <strong>Ge Li</strong>, Hongyi Zhou, Onur Celik, Denis Blessing, Rudolf Lioutikov, Gerhard Neumann, <strong>CoRL, 2024</strong>.
    </p>
    <p>
        See:
        <a href="https://arxiv.org/pdf/2406.08234" target="_blank" rel="noopener noreferrer">arxiv</a> | 
        <a href="https://openreview.net/forum?id=IssXUYvVTg" target="_blank" rel="noopener noreferrer">OpenReview</a> | 
        <a href="https://xiaogangjia.github.io/mail_website/" target="_blank" rel="noopener noreferrer">GitHub</a>   
    </p>
    <p>
        We introduce Mamba Imitation Learning (MaIL), a novel imitation learning architecture that offers a computationally efficient alternative to state-of-the-art Transformer policies.
    </p>
  </div>
</div>

<br><br>

[//]: # (TCE)
<div style="display: flex; align-items: flex-start;">
  <div style="flex: 1; padding-right: 20px; padding-top: 10px;">
    <a href="/assets/images/TCE_abstract.png" target="_blank" rel="noopener noreferrer">
    <img src="/assets/images/TCE_abstract.png" alt="TCE" />
    </a>
    <a href="/assets/images/TCE_cov.png" target="_blank" rel="noopener noreferrer">
    <img src="/assets/images/TCE_cov.png" alt="TCE" />
    </a>
  </div>
  <div style="flex: 2;">
    <p>
        <strong>Open the Black Box: Step-based Policy Updates for Temporally-Correlated Episodic Reinforcement Learning.</strong>
    </p>
    <p>
        <strong>Ge Li</strong>, Hongyi Zhou, Dominik Roth, Serge Thilges, 
        Fabian Otto, Rudolf Lioutikov, Gerhard Neumann, <strong>ICLR, 2024</strong>.
    </p>
    <p>
        See:
        <a href="https://arxiv.org/abs/2401.11437" target="_blank" rel="noopener noreferrer">arxiv</a> | 
        <a href="https://openreview.net/forum?id=mnipav175N" target="_blank" rel="noopener noreferrer">OpenReview</a> | 
        <a href="https://github.com/BruceGeLi/TCE_RL" target="_blank" rel="noopener noreferrer">GitHub</a>   
    </p>
    <p>
        We propose a novel RL framework that integrates step-based information into the
        policy updates of Episodic RL, while preserving the broad exploration scope,
        movement correlation modeling and trajectory smoothness.
    </p>
  </div>
</div>

<br><br>

[//]: # (MP3)
<div style="display: flex; align-items: flex-start;">
  <div style="flex: 1; padding-right: 20px; padding-top: 10px;">
    <a href="/assets/images/mp3_robots.gif" target="_blank" rel="noopener noreferrer">
    <img src="/assets/images/mp3_robots.gif" alt="MP3" />
    </a>
  </div>
  <div style="flex: 2;">
    <p>
        <strong>MP3: Movement Primitive-Based (Re-) Planning Policies</strong>
    </p>
    <p>
        Hongyi Zhou, Fabian Otto, Onur Celik, <strong>Ge Li</strong>, Rudolf Lioutikov, Gerhard Neumann, 
        in Conference on Robot Learning Workshop on Learning Effective Abstractions for Planning, Preprint 2023.
    </p>
    <p>
        See:
        <a href="https://arxiv.org/pdf/2306.12729.pdf" target="_blank" rel="noopener noreferrer">arxiv</a> | 
        <a href="https://intuitive-robots.github.io/mp3_website/" target="_blank" rel="noopener noreferrer">Website</a> |
        <a href="https://drive.google.com/file/d/1ifTGzw1Eocg8YKo4wRiBPtSBUx0dvfzS/view?usp=sharing" target="_blank" rel="noopener noreferrer">Poster</a>
    </p>
    <p>
        We enable a new Episodic RL framework that allows trajectory replanning in deep
        RL, which allows the agent to react with changing goal and dynamic perturbation.
    </p>
  </div>
</div>

<br><br>


[//]: # (ProDMP)
<div style="display: flex; align-items: flex-start;">
  <div style="flex: 1; padding-right: 20px; padding-top: 10px;">
    <a href="/assets/images/robot_pick.gif" target="_blank" rel="noopener noreferrer">
    <img src="/assets/images/robot_pick.gif" alt="ProDMP" />
    </a>
    <a href="/assets/images/prodmp_structure.png" target="_blank" rel="noopener noreferrer">
    <img src="/assets/images/prodmp_structure.png" alt="ProDMP" />
    </a>
  </div>
  <div style="flex: 2;">
    <p>
        <strong>ProDMPs: A Unified Perspective on Dynamic and Probabilistic Movement Primitives.</strong>
    </p>
    <p>
        <strong>Ge Li</strong>, Zeqi Jin, Michael Volpp, Fabian Otto, Rudolf Lioutikov and Gerhard Neumann, in IEEE Robotics and Automation Letters, <strong>RAL 2023</strong>.
    </p>
    <p>
        See:
        <a href="https://ieeexplore.ieee.org/document/10050558" target="_blank" rel="noopener noreferrer">Paper</a> | 
        <a href="https://drive.google.com/file/d/1s3kOqdEiMePO4eefcoBV6mDssjtks8L1/view?usp=drive_link" target="_blank" rel="noopener noreferrer">Poster</a> | 
        <a href="https://github.com/ALRhub/ProDMP_RAL" target="_blank" rel="noopener noreferrer">GitHub</a> |  
        <a href="https://www.youtube.com/watch?v=PAM0NusL2Do&feature=youtu.be" target="_blank" rel="noopener noreferrer">YouTube</a> 
    </p>
    <p>
        We unified the Dynamic Movement Primitives and the Probabilistic Movement
        Primitives into one model, and achieved smooth trajectory generation,
        goal-attractor convergence, correlation analysis, non-linear conditioning, and online
        re-planing in one framework.
    </p>
  </div>
</div>

<br><br>

[//]: # (SVSL)
<div style="display: flex; align-items: flex-start;">
  <div style="flex: 1; padding-right: 20px; padding-top: 10px;">
    <a href="/assets/images/svsl.png" target="_blank" rel="noopener noreferrer">
    <img src="/assets/images/svsl.png" alt="SVSL"/> 
    </a>
    <a href="/assets/images/svsl_obj.png" target="_blank" rel="noopener noreferrer">
    <img src="/assets/images/svsl_obj.png" alt="SVSL"/> 
    </a>
  </div>
  <div style="flex: 2;">
    <p>
        <strong>Specializing Versatile Skill Libraries using Local Mixture of Experts</strong>
    </p>
    <p>
        Onur Celik, Dongzhuoran Zhou, <strong>Ge Li</strong>, Philipp Becker, Gerhard Neumann, 
        in Conference on Robot Learning, <strong>CoRL 2021</strong>. 
    </p>
    <p>
        See:
        <a href="https://proceedings.mlr.press/v164/celik22a/celik22a.pdf" target="_blank" rel="noopener noreferrer">Paper</a> | 
        <a href="https://openreview.net/forum?id=j3Rguo81Yi_" target="_blank" rel="noopener noreferrer">OpenReview</a> |  
        <a href="https://www.youtube.com/watch?v=KQ0ZA-vPCKk&t=478s" target="_blank" rel="noopener noreferrer">YouTube</a> 
    </p>
    <p>
        We developed a mixture of experts RL framework to learn versatile skills
        given the same task, such as forehand and backhand strikes in playing table tennis. Our
        method is able to assign policy experts to their corresponding context domains
        and automatically add or delete these experts when necessary.
    </p>
  </div>
</div>
